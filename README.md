# ğŸš– End-to-End NYC Taxi Data Pipeline with Apache Spark and Delta Lake

This project implements a scalable, end-to-end data pipeline for processing and analyzing New York City (NYC) taxi trip data using **Apache Spark** and **Delta Lake**. The pipeline ingests raw data, applies transformations with performance optimizations, stores results efficiently, and demonstrates advanced features like **Change Data Feed (CDF)** and **time travel**.

---

## ğŸ“š Table of Contents

- Project Goals  
- Technologies Used  
- Architecture & Workflow  
- File Structure & Descriptions  
- Optimizations Implemented  
- Setup Instructions  
- Usage  
- Challenges & Solutions  
- Future Work  
- Contributing  
- License  
- Contact

---

## ğŸ¯ Project Goals

- **Ingest and Process Data**: Load raw NYC taxi trip data and perform transformations.  
- **Optimize Performance**: Apply Spark and Delta Lake optimizations for scalability and efficiency.  
- **Demonstrate Advanced Features**: Leverage Delta Lake's capabilities like CDF, time travel, and schema evolution.  
- **Automate Workflows**: Build a foundation for future automation (e.g., Airflow integration).

---

## ğŸ›  Technologies Used

- **Apache Spark**: Distributed data processing with PySpark (v3.5.0)  
- **Delta Lake**: Optimized storage layer with ACID transactions (v3.2.0)  
- **Python**: Core programming language (v3.10)  
- **Git**: Version control  

---

## ğŸ— Architecture & Workflow

The pipeline follows a modular design with distinct stages:

1. **Ingestion**: Raw data (Parquet and CSV) is ingested into Delta Lake tables  
2. **Exploration**: Initial validation and exploration of ingested data  
3. **Transformation**: Cleaning, enrichment, and aggregation  
4. **Optimization**: Z-ordering, compaction, and partitioning  
5. **Analysis**: Advanced queries and performance testing  

Each stage is implemented via scripts in the `src/` directory, executed sequentially.

---

## ğŸ“ File Structure & Descriptions

<code>
data/
â”œâ”€â”€ delta/                        # Delta tables
â”œâ”€â”€ intermediate/                # Lookup files, e.g., taxi_zone_lookup.csv
â”œâ”€â”€ yellow_taxi_2023_01/         # Raw January trip data
â”œâ”€â”€ yellow_taxi_2023_01_transformed/ # Transformed data output
â”œâ”€â”€ yellow_tripdata_2023-01.parquet

spark_env/
â””â”€â”€ spark_env_310/               # Virtual environment (Python 3.10)

spark-warehouse/                 # Spark SQL metadata and tables

src/
â”œâ”€â”€ 01_ingest.py
â”œâ”€â”€ 02_explore_delta.py
â”œâ”€â”€ 03_analyze.py
â”œâ”€â”€ 03_explore_transformed.py
â”œâ”€â”€ 04_optimize.py
â”œâ”€â”€ 05_test_performance.py
â”œâ”€â”€ 06_test_partitioned.py
â”œâ”€â”€ 07_vacuum.py
â”œâ”€â”€ 08_enable_cdf.py
â”œâ”€â”€ 09_test_cdf.py
â”œâ”€â”€ 10_advanced_analysis.py
â”œâ”€â”€ spark_script.py
â”œâ”€â”€ test_env.py
â””â”€â”€ verify_delta.py

.gitignore                       # Ignore files for Git
requirements.txt                 # Python dependencies
</code>

---

## âš¡ Optimizations Implemented

- **Caching**: In-memory caching of hot DataFrames  
- **Partitioning**: By `pickup_date` for efficient filtering  
- **Z-Ordering**: On `pickup_location_id`, `pickup_date` for skipping  
- **Broadcast Joins**: Small dimension tables avoid shuffles  
- **File Compaction**: `OPTIMIZE` command reduces small file overhead  

These ensure high performance even at scale.

---

## âš™ï¸ Setup Instructions

### 1. Clone the Repository

<code>
git clone https://github.com/udaykotian/Project-Spark-Delta-NYC-Taxi-Data-Pipeline-v2.git
cd Project-Spark-Delta-NYC-Taxi-Data-Pipeline-v2
</code>

### 2. Set Up the Environment

<code>
python3 -m venv spark_env_310
source spark_env_310/bin/activate
pip install -r requirements.txt
</code>

### 3. Download NYC Taxi Data

- Download `yellow_tripdata_2023-01.parquet` and `taxi_zone_lookup.csv` from the NYC TLC website  
- Place both files in the `data/` directory  

---

## â–¶ï¸ Usage

Run scripts in sequence using Spark:

<code>
spark-submit src/01_ingest.py
spark-submit src/02_explore_delta.py
spark-submit src/03_analyze.py
# Continue as needed...
</code>

You can also run individual tasks like:

<code>
spark-submit src/04_optimize.py
</code>

---

## ğŸ§© Challenges & Solutions

- **Data Skew**: Repartitioned during transformations  
- **Small File Problem**: Handled with Delta `OPTIMIZE`  
- **Schema Evolution**: Enabled with `mergeSchema` and autoMerge config  

---

## ğŸš€ Future Work

- Integrate with **Apache Airflow** for scheduling  
- Add **ML models** for predictive insights  
- Add **data quality checks** pre-ingestion  

---

## ğŸ¤ Contributing

All contributions are welcome!

1. Fork this repository  
2. Create a new feature branch  
3. Submit a pull request with a clear description  

---

## ğŸ“„ License

Licensed under the **MIT License**. See `LICENSE` for more.
